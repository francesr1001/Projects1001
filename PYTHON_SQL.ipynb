{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc12d132",
   "metadata": {},
   "source": [
    "SQL IN PYTHON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "connection = oracledb.connect(\n",
    "    user=\"YOUR_USERNAME\",\n",
    "    password=\"YOUR_PASSWORD\",\n",
    "    dsn=\"YOUR_DSN\"\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT country_id, country_name FROM countries\")\n",
    "\n",
    "for row in cursor:\n",
    "    print(row)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54faa5",
   "metadata": {},
   "source": [
    "SEARCHING WITH ENTITY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_USER = \"YOUR_USERNAME\"\n",
    "DB_PASS = \"YOUR_PASSWORD\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"1521\"\n",
    "DB_SERVICE = \"XEPDB1\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"oracle+oracledb://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/?service_name={DB_SERVICE}\"\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT EMPLOYEE_ID,\n",
    "       FIRST_NAME,\n",
    "       LAST_NAME,\n",
    "       EMAIL,\n",
    "       PHONE_NUMBER\n",
    "FROM HR.EMPLOYEES\n",
    "WHERE FIRST_NAME LIKE '%FIND_ENTITY_HERE%'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "df.insert(0, 'Row Count', range(1, len(df) + 1))\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622387a5",
   "metadata": {},
   "source": [
    "WEBSCRAPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "TARGET_URL = \"YOUR_TARGET_WEBSITE\"\n",
    "\n",
    "response = requests.get(TARGET_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "data = {}\n",
    "\n",
    "data['Business Name'] = soup.find('title').text.strip() if soup.find('title') else None\n",
    "\n",
    "services = soup.find_all(['h1', 'h2', 'h3'])\n",
    "data['Type of Services'] = ', '.join(service.text.strip() for service in services if service)\n",
    "\n",
    "email_tag = soup.find('a', href=lambda href: href and 'mailto:' in href)\n",
    "data['Email'] = email_tag['href'].replace('mailto:', '') if email_tag else None\n",
    "\n",
    "about_section = soup.find('div', class_='about-us') or soup.find('section', id='about')\n",
    "data['Long Description'] = about_section.text.strip() if about_section else \"N/A\"\n",
    "\n",
    "phone_tag = soup.find('a', href=lambda href: href and 'tel:' in href)\n",
    "data['Phone #'] = phone_tag.text.strip() if phone_tag else None\n",
    "data['Phone # Link'] = phone_tag['href'] if phone_tag else None\n",
    "\n",
    "whatsapp_tag = soup.find('a', href=lambda href: href and 'wa.me' in href)\n",
    "data['WhatsApp'] = whatsapp_tag['href'].split('wa.me/')[-1] if whatsapp_tag else None\n",
    "data['WhatsApp # Link'] = whatsapp_tag['href'] if whatsapp_tag else None\n",
    "\n",
    "logo_tag = soup.find('img', {'alt': 'logo'}) or soup.find('img')\n",
    "data['Logo Image Link'] = logo_tag['src'] if logo_tag else None\n",
    "\n",
    "address_section = soup.find('address') or soup.find('div', class_='address')\n",
    "data['Address'] = address_section.text.strip() if address_section else \"N/A\"\n",
    "\n",
    "hours_section = soup.find('div', class_='hours') or soup.find('section', id='hours')\n",
    "data['Hours'] = hours_section.text.strip() if hours_section else \"N/A\"\n",
    "\n",
    "facebook_tag = soup.find('a', href=lambda href: href and 'facebook.com' in href)\n",
    "data['Facebook Page Link'] = facebook_tag['href'] if facebook_tag else None\n",
    "\n",
    "instagram_tag = soup.find('a', href=lambda href: href and 'instagram.com' in href)\n",
    "data['Instagram Page Link'] = instagram_tag['href'] if instagram_tag else None\n",
    "\n",
    "for key, value in data.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427fbd",
   "metadata": {},
   "source": [
    "WEBSCRAPING WITH GOOGLE API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual API key\n",
    "SEARCH_ENGINE_ID = \"YOUR_SEARCH_ENGINE_ID\"  # Replace with your actual Search Engine ID\n",
    "\n",
    "urls_to_check = [\n",
    "    \"www.example.com/funding_round/sample-project\", \n",
    "    \"www.example.com/funding_round/another-project\"\n",
    "    # ... Add more as needed\n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "def process_url(query, api_key, search_engine_id):\n",
    "    search_url = f'https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={search_engine_id}'\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    data_item = {'URL': query}\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        if 'items' in response.json():\n",
    "            data_item['Status'] = ''  \n",
    "        else:\n",
    "            data_item['Status'] = 'OOS'\n",
    "    elif response.status_code == 429:\n",
    "        data_item['Status'] = 'Error: 429 for query'\n",
    "        time.sleep(10)  \n",
    "    else:\n",
    "        data_item['Status'] = f\"Error: {response.status_code}\"\n",
    "\n",
    "    return data_item\n",
    "\n",
    "for query in urls_to_check:\n",
    "    data_item = process_url(query, API_KEY, SEARCH_ENGINE_ID)\n",
    "    data.append(data_item)\n",
    "\n",
    "valid_urls = [item['URL'] for item in data if (item['Status'] == '' or 'Error' in item['Status']) and item['Status'] != 'OOS']\n",
    "\n",
    "formatted_urls = '\",\"'.join(valid_urls)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_file = \"websites_status_single_api_key.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
